---
layout: page
title: Parallel Computing on HPC systems
subtitle: Week 1, Session 1, Hour 1 : Getting on and around HPC systems
minutes: 60
---
> ##  Outline {.objectives}
> * Research Computing in Canada and at Queen's
> * The Frontenac Compute Cluster
> * Accessing HPC systems
> * Checking out the system
> * Getting around using bash

## Research Computing in Canada

We give a short overview on Canadian academic research computing in a slide deck.
This inculdes:

* Compute Canada (national organization)
* Compute Ontario (provincial organization)
* Individual "Consortia" and Compute Centres
* National platform systems
* The Centre for Advanced Computing
* The Frontenac Compute Cluster

## What is an HPC system?

The words “cloud”, “cluster”, and “high-performance computing” are
used a lot in different contexts and with varying degrees of
correctness. So what do they mean exactly? And more importantly, how
do we use them for our work?

The cloud is a generic term commonly used to refer to remote computing
resources of any kind that is, any computers that you use but are not
right in front of you. Cloud can refer to machines serving websites,
providing shared storage, providing webservices (such as e-mail or
social media platforms), as well as more traditional “compute”
resources. An HPC system on the other hand, is a term used to describe
a network of computers. The computers in a cluster typically share a
common purpose, and are used to accomplish tasks that might otherwise
be too big for any one computer.

## Login

Let's log in to the cluster: Frontenac at Queen's

~~~~ {.python}
$ ssh yourUsername@login.cac.queensu.ca
~~~~
~~~~ {.output}
Last login: Thu Nov 28 08:26:42 2019 from s2-n59.gen.queensu.ca

===============================================================================
Welcome to the CAC cluster Frontenac!

For more info on using this system, please see our wiki at:
https://cac.queensu.ca/wiki/index.php/Frontenac

If you run into any issues, please email us at cac.help@queensu.ca.
-------------------------------------------------------------------------------

Note that this node is for workup, testing, and job submission only.
All production jobs must be submitted to the Frontenac cluster through
the SLURM scheduler (https://cac.queensu.ca/wiki/index.php/SLURM).
User jobs will be terminated without notice.

===============================================================================
~~~~

Remember to replace yourUsername with the username supplied by the instructor. You will be asked for your password. But watch out, the characters you type are not displayed on the screen.

You are logging in using a program known as the secure shell or ssh. This establishes a temporary encrypted connection between your laptop and the "login node: of the Frontenac compute cluster. The word before the @ symbol, e.g. yourUsername here, is the user account name that Lola has access permissions for on the cluster.

## Secure Shell (ssh)

ssh stands for "secure shell". It is a protocoll that uses a decoded data stream. The "ssh" utility is used for login. Others are used for data transfer : "sftp" and "scp" (for "secure file transfer protocol" and "secure copy".

On Linux and/or macOS, the ssh command line utility is almost always pre-installed. Open a terminal and type ssh --help to check if that is the case.

For Windows systems we recommend MobaXterm. Download it, install it and open the GUI. The GUI asks for your user name and the destination address or IP of the computer you want to connect to. Once provided, you will be queried for your password.

## Basic orientation

Very often, users are tempted to think of a high-performance
computing installation as one large machine. Sometimes,
people will assume that the computer they’ve logged onto is the entire
computing cluster. So what’s really happening? What computer have we
logged on to? The name of the current computer we are logged onto can
be checked with the hostname command. (You may also notice that the
current hostname is also part of our prompt!)

~~~~ {.python}
$ hostname
caclogin03
~~~~

## Nodes

Individual computers that compose a cluster are typically called nodes
(although you will also hear people call them servers, computers and
machines). On a cluster, there are different types of nodes for
different types of tasks. The node where you are right now is called
the head node, login node or submit node. A login node serves as an
access point to the cluster. As a gateway, it is well suited for
uploading and downloading files, setting up software, and running
quick tests. It should never be used for doing actual work.

The real work on a cluster gets done by the worker (or compute)
nodes. Worker nodes come in many shapes and sizes, but generally are
dedicated to long or hard tasks that require a lot of computational
resources.

All interaction with the worker nodes is handled by a specialized
piece of software called a scheduler (the scheduler used in this
lesson is called ). We’ll learn more about how to use the scheduler to
submit jobs next, but for now, it can also tell us more information
about the worker nodes.

For example, we can view all of the worker nodes with the sinfo command.

~~~~ {.python}
$ sinfo
~~~~
~~~~ {.output}
PARTITION   AVAIL  TIMELIMIT  NODES  STATE NODELIST
standard       up 14-00:00:0      5    mix cac[074-076,079,081]
standard       up 14-00:00:0      1  alloc cac080
standard       up 14-00:00:0      2   idle cac[077-078]
reserved*      up 28-00:00:0      1   resv cac106
reserved*      up 28-00:00:0     19    mix cac[025-026,028-033,055-060,074-076,079,081]
reserved*      up 28-00:00:0      3  alloc cac[035-036,080]
reserved*      up 28-00:00:0     33   idle cac[037-054,061-073,077-078]
orc            up 28-00:00:0      2  drain cac[118,140]
orc            up 28-00:00:0     18    mix cac[111-117,119,141-150]
orc            up 28-00:00:0     20   idle cac[120-139]
gpu            up 14-00:00:0      3   idle cac[107-109]
power-gpu      up 14-00:00:0      1  drain cac151
power-gpu      up 14-00:00:0      4   idle cac[152-155]
gpu-vin        up 14-00:00:0      1   idle cac110
interactive    up    8:00:00      1   idle cac034
debug          up 14-00:00:0     19    mix cac[025-026,028-033,055-060,074-076,079,081]
debug          up 14-00:00:0      3  alloc cac[035-036,080]
debug          up 14-00:00:0     34   idle cac[034,037-054,061-073,077-078]
~~~~

There are also specialized machines used for managing disk storage,
user authentication, and other infrastructure-related tasks. Although
we do not typically logon to or interact with these machines directly,
they enable a number of key features like ensuring our user account
and files are available throughout the HPC system.

## What is inside a node ?

All of a HPC system’s nodes have the same components as your own
laptop or desktop: CPUs (sometimes also called processors or cores),
memory (or RAM), and disk space. CPUs are a computer’s tool for
actually running programs and calculations. Information about a
current task is stored in the computer’s memory. Disk refers to all
storage that can be accessed like a file system. This is generally
storage that can hold data permanently, i.e. data is still there even
if the computer has been restarted.

We can get information about the headnode using system query
commands. Here are a few examples. To determine the available number
of processors, for instance:

~~~~ {.python}
$ nproc --all
24
~~~~
~~~~ {.output}
24
~~~~

To show details about the CPU's on the system:

~~~~ {.python}
$ lscpu
~~~~
~~~~ {.output}
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                24
On-line CPU(s) list:   0-23
Thread(s) per core:    1
Core(s) per socket:    24
Socket(s):             1
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 85
Model name:            Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz
Stepping:              4
CPU MHz:               2294.673
BogoMIPS:              4589.32
Hypervisor vendor:     Xen
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              1024K
L3 cache:              16896K
NUMA node0 CPU(s):     0-23
Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch intel_ppin fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm xsaveopt
~~~~

If you want to know how much memory you have available, you can use

~~~~ {.python}
$ free -m
~~~~
~~~~ {.output}
              total        used        free      shared  buff/cache   available
Mem:         161010       97483       60011         402        3514       62001
Swap:          8191        7135        1056
~~~~

For even greater detail about memory you can use the command

~~~~ {.python}
$ cat /proc/meminfo
~~~~ 

## What about the other nodes ?

Of course the above commands are issued at the commandline, and apply to the login node on which you are currently working. To get information about the cluster nodes you need to ask the scheduler. The sinfo command has many options that allow you to get details about a specific node. A basic quick look at cac108 can be had this way:

~~~~ {.python}
$ sinfo -n cac108 -o "%n %c %m"
~~~~ 
~~~~ {.output}
HOSTNAMES CPUS MEMORY
cac108 32 180000
~~~~

So this particular node (cac108) has 32 CPU's and 180 GB of memory (base usint at Megabyte).


