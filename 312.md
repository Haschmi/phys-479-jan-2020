---
layout: page
title: Parallel Computing on HPC systems
subtitle: 2D heat equation, untreated topics
minutes: 60
---

> ##  Outline
> * Extensions to 2D and 3D
> * Cyclic block distribution
> * Stuff we have not touched

## Finite differences in more than 1D

The extension of the simple approximation of the 1D heat equation to
more than 1 dimension is rather straight-forward. Assuming that we
have the same heat conductivity in both directions, and still ignoring
any source terms we have:

~~~~{.python}
du/dt = k * (d2u/dx^2 + d2u/dz^2)
~~~~

which can be discretized the same way as in one direction, just
yielding a somewhat longer expresssion

~~~~{.python}
[u(n+1,i,j) - u(n,i,j)]/dt =
K * {[u(n,i,j+1)-2*u(n,i,j)+u(n,i,j-1)]/(dx)^2} +
     [u(n,i+1,j)-2*u(n,i,j)+u(n,i-1,j)]/(dy)^2}
~~~~
This assumes an isotropic Cartesian grid, and since it's best to use
the same spacing in both direction, we may as well set dx=dy=d. The
indices j and i are counting the gridpoints in x and y direction,
respectively. Everything else is like in 1D. In the case of three
dimensions we'd have three terms and indices, but the basic form is
the same.

If the conductivity term K is not the same for the directions, or if
the grid is not chosen equal in all directions, things get more
complicated. Also, the approximation of the Laplacian by a simple
thre-point formula may not be accurate enough. In several dimensions
things may be inhomogeneous enough to warrant a different coordinate
system than a simple Cartesian one. The situation may make itr
impossible to properly separate variables, etc.

Still, the principle remains the same:

* approximate the continuous solution of the differential equation by
  discret points on a grid

* write the derivatives as an expression in terms of the gridpoints
  and nearby "neighbours"

* solve for the value of the gridpoints in the "next" time step

* initialize with the stareting distribution

* enforce boundary conditions at each step

## Domain decomposition in 2D and 3D

Domain decomposition is easy on a Cartesian grid. If the grid is
homogeneous, isotropoic, and Cartesian, we can limit the number of
data that have to be fetched in each step by, for instance distribute
the workload (and memory) only in one direction. For a 3D domain, this
corresponds to "slabs", for 2D it's "strips". For some cases, the Grid
can't be chosen to be homogeneous, though. For more complicated
domains or more sophisticated dynamics, there may be regions of "more
action" that have to be covered by a denser grid. Or worse, the time
scale of what's going on may be different. The potential for
complexity is unlimited.

Much of the effort of working with grid based methods in more complex
cases goes into constructing a grid that covers the regions and time
scales of interest in such a way that the computational load on each
of the processes is approximately equal. Because you are on a time
scale, the iterations are dependent, and you may have to "redraw" the
grid every so often to adapt to the changing circumstances.

## Assignment: Parallizing 2D equation

The following is a code for the 2D heat equation (serial).
The time step size is chosen maximal to maintain stability.
Thge code was modified from:

https://scipython.com/book/chapter-7-matplotlib/examples/the-two-dimensional-diffusion-equation/

~~~~{.python}
#! /bin/env python

import numpy as np
import matplotlib.pyplot as plt

# plate size, mm
w = h = 20.
# intervals in x-, y- directions, mm
dx = dy = 0.1
# Thermal diffusivity of steel, mm2.s-1
K = 4.2

Tcool, Thot = 300, 1000

nx, ny = int(w/dx), int(h/dy)

dx2, dy2 = dx*dx, dy*dy
dt = dx2 * dy2 / (2 * K * (dx2 + dy2)) # max timestep before process becomes unstable

u0 = Tcool * np.ones((nx, ny))
u = np.empty((nx, ny))

# Initial conditions - ring of inner radius r, width dr centred at (cx,cy) (mm)
r, cx, cy = 5, 10, 10
r2 = r**2
for i in range(nx):
    for j in range(ny):
        p2 = (i*dx-cx)**2 + (j*dy-cy)**2
        if p2 < r2:
            radius = np.sqrt(p2)
            value = 2*np.cos(4*radius)**4
            u0[i,j] = value*Thot

def do_timestep(u0, u):
    # Propagate with forward-difference in time, central-difference in space
    for i in range(1,nx-1):
        for j in range(1,ny-1):
            u[i,j] = u0[i,j] + K * dt * (
                (u0[i+1, j] - 2*u0[i, j] + u0[i-1, j])/dx2
              + (u0[i, j+1] - 2*u0[i,j] + u0[i, j-1])/dy2 )

    u0 = u.copy()
    return u0, u

# Number of timesteps
nsteps = 100
# Output 4 figures at these timesteps
for m in range(nsteps):
    u0, u = do_timestep(u0, u)
~~~~

The boundary conditions are such that a low temperature is maintained
at the edges of the square domain. Initial conditions apply to a
circular are in the centre of the domain where the temperature
"oscillates" between maximal and minimal temperature. Time
developments will tend to wash out these features. The assignment is
to parallelize the code and determine the scaling properties of the
resulting code.

Get details from the posted assignment.

## Cyclic Block distribution

In our one-dimensional example, we are splitting up the domain into
approximately equal "chunks" and have each of the processes tackle on
of those segments. Sometimes this appraoch may not be desirable, as it
can lead to an uneven workload, for instance if the required
computations for small grid indices are simpler than for large
ones. In that case we would prefer to choose small block sizes and
give each of the processes more than on block to work through. Clearly
this has the downside of requireing more communication, as each of the
blocks has to communicate with its neighbours, irrespective of the
size: more blocks, more commuinication. But it may be worth it. We
would be distributing the blocks in a "round-robin" way,
i.e. cyclically:

<img src="fig/1d-cbd.PNG" width="600">

For a two dimensional domain, we can do the distribution in one
direction only, or we can overlay two of these "cyclic blkock
distributions" in both directions. The result is a checker-board like
pattern:

<img src="fig/2d-cbd.PNG" width="600">

Each of the colors in the above pictures correspond to a different
process that does the computations (and usually receives the data) on
the corresponding blocks. This can be extended to three dimensions (or
more), but let's leave it at this.

While the base pattern here is fairly simple, the computations
required to compute the proper indices for a call to "Scatterv" or a
similar function, are rather tedious. Standard distribution patterns
such as this are often done using functions from parallel computing
packages such as ScaLapack. These are doing the MPI calls "under the
hood", and you use a call to a higher-level function to specify the
basic parameters, such as the size of the full array, the number of
processes, and the blocksize in each direction. The rest happens
automatically.

## MPI issues we have not discussed

Here is a list of some topics we have not discussed but they may
warrant some research :

* User defined data types : The standard data types used for MPI
  communication are arrays. More complicated data structure
  (especially "mixed" ones) require specific declaration.

* Multiple communicators : We have only used MPI.COMM_WORLD as the
  default communicator. If there are more than one level of
  communication, it is necessary to generate new communicators to
  group the processes.

* Parallel I/O : If the workload contains a substantial I/O component
  then it is necessary to do htese in parallel. That requires
  simultaneous access by multiple processes to the same file. MPI has
  many functions doing this.

* There are many more advanced topics, such as structured
  communication, one-sided communication, combination of MPI with
  other forms of parallism, such as multithreading.

