---
layout: page
title: Parallel Computing on HPC systems
subtitle: TBD
minutes: 60
---
> ##  Outline {.objectives}
> * Extensions to 2D and 3D
> * Cyclic block distribution
> * Stuff we have not touched
> * Higher-Level packages and libraries

## Finite differences in more than 1D

The extension of the simple approximation of the 1D heat equation
to more than 1 dimension is rather straight-forward. Assuming that we have the same heat conductivity in both directions, and still ignoring any source terms we have:

du/dt = k * (d2u/dx^2 + d2u/dz^2)

which can be discretized the same way as in one direction, just
yielding a somewhat longer expresssion

[u(n+1,i,j) - u(n,i,j)]/dt =
k * {[u(n,i,j+1)-2*u(n,i,j)+u(n,i,j-1)]/(dx)^2} +
     [u(n,i+1,j)-2*u(n,i,j)+u(n,i-1,j)]/(dz)^2}

This assumes an isotropic Cartesian grid, and since it's best to use
the same spacing in both direction, we may as well set dx=dz=d. The
indices j and i are counting the gridpoints in x and z direction,
respectively. Everything else is like in 1D. In the case of three
dimensions we'd have three terms and indices, but the basic form is
the same.

If the conductivity term k is not the same for the directions, or if
the grid is not chosen equal in all directions, things get more
complicated. Also, the approximation of the Laplacian by a simple
thre-point formula may not be accurate enough. In several dimensions
things may be inhomogeneous enough to warrant a different coordinate
system than a simple Cartesian one. The situation may make itr
impossible to properly separate variables, etc.

Still, the principle remains the same:

* approximate the continuous solution of the differential equation by discret points on a grid
* write the derivatives as an expression in terms of the gridpoints and nearby "neighbours"
* solve for the value of the gridpoints in the "next" time step
* initialize with the stareting distribution
* enforce boundary conditions at each step

## Domain decomposition in 2D and 3D

Domain decomposition is easy on a Cartesian grid. If the grid is
homogeneous, isotropoic, and Cartesian, we can limit the number of
data that have to be fetched in each step by, for instance distribute
the workload (and memory) only in one direction. For a 3D domain, this
corresponds to "slabs", for 2D it's "strips". For some cases, the Grid
can't be chosen to be homogeneous, though. For more complicated
domains or more sophisticated dynamics, there may be regions of "more
action" that have to be covered by a denser grid. Or worse, the time
scale of what's going on may be different. The potential for
complexity is unlimited.

Much of the effort of working with grid based methods in more complex
cases goes into constructing a grid that covers the regions and time
scales of interest in such a way that the computational load on each
of the processes is approximately equal. Because you are on a time
scale, the iterations are dependent, and you may have to "redraw" the
grid every so often to adapt to the changing circumstances.

## Cyclic Block distribution

In our one-dimensional example, we are splitting up the domain into
approximately equal "chunks" and have each of the processes tackle on
of those segments. Sometimes this appraoch may not be desirable, as it
can lead to an uneven workload, for instance if the required
computations for small grid indices are simpler than for large
ones. In that case we would prefer to choose small block sizes and
give each of the processes more than on block to work through. Clearly
this has the downside of requireing more communication, as each of the
blocks has to communicate with its neighbours, irrespective of the
size: more blocks, more commuinication. But it may be worth it. We
would be distributing the blocks in a "round-robin" way,
i.e. cyclically:

<img src="fig/1d-cbd.png" width="600">

For a two dimensional domain, we can do the distribution in one
direction only, or we can overlay two of these "cyclic blkock
distributions" in both directions. The result is a checker-board like
pattern:

<img src="fig/2d-cbd.png" width="600">

Each of the colors in the above pictures correspond to a different
process that does the computations (and usually receives the data) on
the corresponding blocks. This can be extended to three dimensions (or
more), but let's leave it at this.

While the base pattern here is fairly simple, the computations
required to compute the proper indices for a call to "Scatterv" or a
similar function, are rather tedious. Standard distribution patterns
such as this are often done using functions from parallel computing
packages such as ScaLapack. These are doing the MPI calls "under the
hood", and you use a call to a higher-level function to specify the
basic parameters, such as the size of the full array, the number of
processes, and the blocksize in each direction. The rest happens
automatically.

## MPI issues we have not discussed

(...to be added...)

## Higher-level packages and libraries

(...to be added...)

