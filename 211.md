---
layout: page
title: Parallel Computing on HPC systems
subtitle: Week 2, Session 1, Hour 1 : Basic MPI, Point-2-Point Communication
minutes: 60
---
> ##  Outline {.objectives}
> * Basic features of MPI : Communicators
> * Simple MPI data types, MPI4Py
> * Simple MPI functions
> * Point-2-Point Communication
> * Blocking and non-blocking communication, Wait
> * Collective Operations

The subjects in this section are covered by a slide presentation.
The followwing is largely taken from a recent "MPI for python" workshop.

MPI was originally designed for FORTRAN and C. Later (in version 2)
C++ was added. Meanwhile, some implementations of MPI cover Java. It
was only a matter of time that someone would supply us with a Python
version. The MPI interfaces we will work with are called <a
href="http://pythonhosted.org/mpi4py/">MPI4Py</a>. This is not the
only package with this purpose, but it is simple to use and therefore
ideally suited for an introductory course.

One nice thing about MPI4Py is that many simple applications don't
require as detailed specification as is needed for the native bindings
of MPI. For beginners this means that they don't have to worry about a
lot of details that are required when dealing with MPI in C or C++. On
the other hand, once moving on to bigger projects, some of these
details become important and one has to re-visit them.

## What we will use

Here's a list of MPI functions that we will be using in our next few
mini-projects. They are the absolute minimum needed to write a working
MPI program

| Action | Name of Command | Simplest Implementation |
|-----------|----------|-----------------------------------------|
Package Load| | from mpi4py import MPI 
Initialization | MPI_Init | Not required in Python, happens when loading MPI4Py
Finalization | MPI_Finalize | Not required, happens automatically at program end
Communicator | Set directly | comm=MPI.COMM_WORLD
Rank | MPI_Comm_rank | comm.Get_rank()
Size | MPI_Comm_size | comm.Get_size()
Send (blocking) | MPI_Send | comm.Send(data,  dest=drank, tag=itag) (automatic)
Receive (blocking) | MPI_Recv | comm.Recv(data, source=srank, tag=itag) (automatic)
Send (non-blocking) | MPI_Isend | comm.Send(data,  dest=drank, tag=itag) (automatic)
Receive (non-blocking) | MPI_Irecv | comm.Recv(data, source=srank, tag=itag) (automatic)
Wait (blocking) | MPI_Wait | req.Wait() 
Test (blocking) | MPI_Test | req.Test() 
Broadcast | MPI_Bcast | comm.Bcast(data, root=rrank) (automatic)
Reduce | MPI_Reduce | comm.Reduce(pdata, tdata, op=MPI.op, root=rrank) (automatic)
Barrier | MPI_Barrier | Comm.Barrier()

## Initialization and Finalization

There is an actual intialization routine included (Init()) but we
won't have to use it, as it automatically called when we load the
MPI4Py package. Youi can check with MPI.Is_initialized(). If you're
trying to do the initialization again, it throws an error:

~~~ {.python}
from mpi4py import MPI
MPI.Is_initialized()
~~~
~~~ {.output}
True
~~~
~~~ {.python}
MPI.Init
~~~
~~~ {.error}
--------------------------------------------------------------------------
Calling MPI_Init or MPI_Init_thread twice is erroneous.
--------------------------------------------------------------------------
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "MPI/MPI.pyx", line 113, in mpi4py.MPI.Init (src/mpi4py.MPI.c:144518)
mpi4py.MPI.Exception: MPI_ERR_OTHER: known error not in list
~~~

Same with Finzalization: not need to do it explicitely, as it is
automatically done when the program quits. If you must for some reason
you can with MPI.Finalize(). Of course, you can't finalize twice
either. In fact trying to do that crashes the whole program:

~~~ {.python}
MPI.Finalize()
MPI.Is_finalized()
~~~
~~~ {.output}
True
~~~
~~~ {.python}
MPI.Finalize()
~~~
~~~ {.error}
*** The MPI_Finalize() function was called after MPI_FINALIZE was invoked.
*** This is disallowed by the MPI standard.
*** Your MPI job will now abort.
[(null):17185] Local abort after MPI_FINALIZE completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
~~~

## Communicators, Rank and Size

As mentionedd before, Rank and Size are "communicator specific". Communicators are easily specified in MPI4Py. Just set a variable to the communicator you want to use, and call all functions as members of that communicator:

~~~ {.python}
from mpi4py import MPI
mycomm=MPI.COMM_WORLD
print ("My rank is ",mycomm.Get_rank())
~~~
~~~ {.output}
My rank is  0
~~~
~~~ {.python}
print ("The size is ",mycomm.Get_size())
~~~
~~~ {.output}
The size is  1
~~~

## Send and Receive

We're only going to deal with blocking send/receive calls, as they are
fairly simple. We can even test them out from inside the interpreter
with one process only who sends itself a message:

~~~ {.python}
message=657
mycomm.send(message, dest=0, tag=11)
print("The received message is ",mycomm.recv(source=0, tag=11))
~~~
~~~ {.output}
The received message is  657
~~~

Sort of silly, but it illustrates the principle. Careful with the
tag. If the one in the receive doesn't match the one in the send,
you're sitting there 'til kingdom come:

~~~ {.python}
mycomm.send(message, dest=0, tag=11)
print("The received message is ",mycomm.recv(source=0, tag=12))
~~~
~~~ {.output}
[...nothing to see here, folks...]
~~~

Now we should be ready to do some communication. We can re-cycle some
of the stuff we typed into hello.py. So let's copy it

~~~ {.python}
cp hello.py p2p.py
~~~

and then edit p2p.py. Let's get rid of the output line at the end, so
we have:

~~~ {.python}
#!/usr/bin/env python3
from mpi4py import MPI
comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()
~~~

Let each process compute the square of it's rank and then send that
message to rank0. This has rank 0 go through a loop over all numbers
between 1 and the size-1

~~~ {.python}
if rank == 0:
    for index in range(1,size):
        data = comm.recv(source=index, tag=11)
        print (" Received from process ",index," : sq = ",data)
~~~

We're specifying the "source" of the message to be the loop
index. We've arbitrarily chosen the tag to be "11" because there is
only one message between a given pair of processes, so there's no
danger of getting messages mixed up. Make sure you do the indentation
right, though.  So this is what process 0 does. The others do the
computation of the squares and a send, i.e.

~~~ {.python}
else:
    sq=rank*rank
    comm.send(sq, dest=0, tag=11)
~~~

Make sure you have blank line above the "else" so that the loop is
properly closed!  The tag is "11" again to make a match, and the
destination is the root process rank 0. We don't have to specify
length and type of the messages because Python is smart enough to
figure that out.

~~~ {.python}
$ mpirun -np 8 ./p2p.py
 Received from process  1  : sq =  1
 Received from process  2  : sq =  4
 Received from process  3  : sq =  9
 Received from process  4  : sq =  16
 Received from process  5  : sq =  25
 Received from process  6  : sq =  36
 Received from process  7  : sq =  49
~~~

Note how everything is nicely in order. That is because we have rank 0
go through the loop one by one, i.e. it talks with one process after
the other. If that sounds serial to you it's because it is. This is a
general issue: communication tends to make things serial, so you
should have as little of it as possible. And if you must have it, it's
best to use collective operations where stuff happens inside a routine
and chances are serializations such as this one has been kept to a
minimum. There's tricks to do this but that's a bit beyond our scope
here.

## Blocking and non-blocking communication (P2P)

The point-to-point communication we use in the above example is called
"blocking". This means that once a call to a communication function
(send or recv) is made, the program flow on the process that makes it
does not return to the calling routine before the operation is
finished. This is particularly important in the case of a "receive"
call. If a process is ready to receive a message and calls comm.recv()
it will wait until the "receive buffer" contains the message,
i.e. until it's been delivered. The waiting time can be substantial,
as the successful reveive operation depends on the corresponding send
call on another process.

The upside of this "wait until delivered" approach is that it's safe:
we are not tempted to do something with the array that is supposed to
contain a message before it actually does, because we can't.

The downside is it tends to slow the program flow. While we are
waiting for the delivery of the message, we could be doing something
else that does not involve the expected message. To enable that, there
is a "non-blocking" version of the receive, called Irecv(). A call to
Irecv just readies an array that will contain the message, but does
not wait around until it's actually there. Instead the process returns
to the calling routine and is free to "do something else". The return
value of an Irecv call is a so-called "request". This is an object
that can be used to check later if the message has been received.

The methoid how to check is a call to the method "Wait". The process
returns from that call only if the message was received, otgherwise it
remains in the call until it has. Let's play through the mechanics of
this with a sibngle-process session of python:

~~~~
hasch@caclogin02$ python
Python 3.5.2 (default, Jun 25 2016, 21:38:40)
[GCC 5.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> from mpi4py import MPI
>>> import numpy as np
>>> comm=MPI.COMM_WORLD
>>> rn=np.random.random_sample(1)
>>> print(rn)
[ 0.2858589]
>>> reqs=comm.Isend(rn,dest=0)
>>> reqs.Wait()
>>> reqr=comm.Irecv(rnr,source=0)
>>> reqr.Wait()
>>> print(rnr)
[ 0.2858589]
>>> quit()
~~~~

We're creating a "numpy" random number and sending it non-blocking
with Isend(). That gives us a request reqs which we are checking out
with reqs.Wait(). Likewise with the receive Irecv and the
corresponding request reqr. In this case, we're sending messages to
ourselves which is a bit boring, but again, it demonstrates the
principle.

The point of this is that a blocking receive would "trap" the program
flow in Recv(), while in the non-blocking one traps it in Wait(). That
opens a gap between Irecv() and Wait() which we can use to do other
things that don't require the communicated data (rnr in our example).

If additional flexibility is required, we can call Test() instead of
Wait(). This does not trap the program flow but rather returns "True"
or "False" depending on whether the communication operation associated
with the request has finished or not. Like this (for a Send):

~~~~
>>> from mpi4py import MPI
>>> import numpy as np
>>> comm=MPI.COMM_WORLD
>>> rn=np.random.random_sample(1)
>>> req=comm.Isend(rn,dest=0)
>>> req.Test()
True
>>> quit()
~~~~

This way, we can go through repeated Test / do something else cycles
until the Send or Recv are finished.

## Collective Operations :  Broadcast, Reduce, Barrier

We have already encountered two collective operations in the
sum-of-square roots example. Collective operations always involve all
the processes in a communicator, and they are always blocking. This means that:

* Every process must call a collective operation for it to finish successfully.
* Processes are "trapped" in a collective function call until it is completed.

As a result, an importan rule is:

* Never put a collctive operation into a place where some of the
  processes may not reach it, for instance into an "if" clause.

Since collective operations require more than one process to
demonstrate, let's postpone that to the next lessons. However, there's
one thing we need to bring up here: the simplest way to use collective
communication in MPI4Py is by asending stuff as arrays. So even if
it's just a simple number you want to broadcast (i.e. to send from one
process to all the others) it's best to do something like:

~~~ {.python}
import numpy as np
number=np.zeros(1)
comm=MPI.COMM_WORLD
comm.Bcast(number, root=0)
~~~

Of, course nothing new happens since a single process "broadcasts" a
single number to itself. But at least it doesn't crash. Note that
we're using np.zeroes(1) to define the shape of number, i.e. an array
of length 1, i.e. a single number.

An operation we have not encountered yet is a Barrier:

~~~ {.python}
comm.Barrier()
~~~

A barrier causes each process to be trapped in the call until all
processes in the communicator have made the call. This is often used
to "synchroize" all processes, i.e. to make sure that they all have
reached a certain point in the program. Barriers may be instered in
places where they (technically speaking) are not essential, just tom
ensure that the program flow on the different processes does not get
too asynchroneous, whcih may lead to lockups.

Note that for most of these funtion calls there are several ways of
specifying details by adding more arguments. For how to use this, you
need to study the package in detail. The most compelling aspect of
these interfaces is that Python usually can figure out what is the
type and shape of the data that are being transmitted. As long as
things match on both sides of the communication, you're usually good.

