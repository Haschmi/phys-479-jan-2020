---
layout: page
title: Parallel Computing on HPC systems
subtitle: Week 3, Session 1, Hour 1 : Domain Decomposition and Heat Equation
minutes: 60
---
> ##  Outline
> * Domain decomposition
> * General considerations
> * Finite Differences
> * Simple example : 1D Heat Equation
> * Bare Bones Implementation

This part of the markup pages is incomplete.

## Finite differences

Many of the problems in dynamics are expressed in terms of
differential equations, i.e. they express relationship between
derivatives of physical quantities with respect to space and time. In
order to solve suchg equations on a computer, the typical approach is
to discretize the continuous solution of the differential equation on
a "grid", i.e. to work with discrete values rather than an explicit
expression for the function.

We then approximate the spatial derivatives that occur in the
differential equation by expresssions based on the difference of the
functional values at neighboring points of the grid. This results in
linear equation systems that can be solved to obtain consecutive
values in time.

Details for this concern the level of approximation for the spatial
derivatives, and largely influence how many points on the grid have to
be used to approximate first and second derivatives.

In this session we will look at a simple diffusion (or heat)
equation, where the rate of change in the quantity under consideration
(say, heat) is proportional to the second derivative (curvature) of
the quantity at that point. The curvature can be thought of as the
difference between the quantity at the point and its average in an
infinitesimal neighborhood. The bigger the difference, the quicker it changes.

~~~~{.python}
du/dt = k * d2u/dx2
~~~~

Commonly, there are boundary conditions that fix the value and/or it's
derivatives at the boundary of the domain in question.

Let's write the above equation in terms of "finite differences",
i.e. neighboring points of the one-dimensional domain. We use the
"forward Euler scheme" for this, i.e. the time derivative is written as a difference between the next point in time and the current one. The second derivative is expressed by an approximation using the left- and right-hand neighbors and the value at the point itself:

~~~~{.python}
[ u(i,n+1)-u(i,n) ] / dt = k * [ u(i+1,n) - 2 * u(i,n) + u(i-1,n) ] / (dx)^2
~~~~

We used the following notations:
* index i labels a point in space, all equidistant with distance dx
* index n labels the time, starting with 0 and time steps of dt

That's as simple and Cartesian as we can make it. We can easily solve
this for the "next" time step:

~~~~{.python}
u(n+1,i) = u(n,i) + F * (u(n,i+1) - 2(n,i) + u(n,i-1))

where F = k*dt/(dx)^2
~~~~

This gives us a recipe on how to compute the time development of a
discretized heat distribution in one dimension. It will be pretty
inaccurate if few gridpoint in space are used, but if we choose them
dense enough, the derivatives are reasonable and the scheme will work.

## Domain decomposition

For a one-dimensional scheme this is probably best done in serial, but
for 2D and 3D problems, issue may appear rather quickly: a hundred
points in each direction of 3D space, and you have to deal with a
million grid points. The expressions for the derivatives become more
involved, and more neighbours may have to be drawn in to increase
accuracy. Too many gridpoints, and the memory requirements become too
large to be kept on a single machine.

The way to make the problem tractable may be to split up the "domain",
i.e. the space segement under consideration into several pieces, each
tackled on a different CPU, or node. This is called domain
decomposition and is the standard method of parallelizing grid-based
methods. But there is a catch: For each time step, some of the values
that are needed to compute the second derivative may not be part of
the domain a specific process is responsible for. Otherwise we could
just solve a hundred independent versions of the equation in 100
subdomains independently. It's almost the same thing, but the boundary
conditions for the subdomains change at every time step and have to be
communicated every time.

So typically, with parallel finite-difference methods, every time step
reuires communication with the nearest neighbors. For Cartesian grids
these are typically two for 1D, 4 for 2D, and 6 for 3D. The more
processes, the more subdomains, the more communication, the more
overhead. At least the communication scales linearly with the number
of processes, as not everyone has to talk with everyone. And at the
overall boundaries, the conditions are fixed.

## Explicit implementation

The following is an implemantation of the 1D heat equation that was
converted from the C version of John Burkhardt
(https://people.sc.fsu.edu/~jburkardt/c_src/heat_mpi/heat_mpi.html)

This version also includes a "source term" on the right-hand side of
the differential equation. Such source terms are common and correspond
(as the name suggests) to external time dependent sources of
heat. This term does not complicate the picture much, but we kept it
out of our introduction. I appears as the array rhs[] in the code.

~~~ {.python}
#! /usr/bin/env python3

import cmath
import time as tm
import matplotlib.pyplot as plt
import numpy as np
import sys

from mpi4py import MPI

def timestamp() :
    print ( "Local time : ", tm.ctime( tm.time() ) )
    return

def initial_condition ( x, time ):
    value = 95.0
    return value

def rhs ( x, time ) :
    value = 0.0
    return value

def boundary_condition ( x, time) :

# left and right boundary conditions
# the left one is time dependent, the right one is constant
    
    if ( x < 0.5 ) :
        value = 100.0 + 10.0 * np.sin ( time )
    else :
        value = 75.0
    
    return value

# This routine does almost all of the work:
# For each process, it runs through all time steps, updating H
# For one process, results are printed out

def update(id,p) :

    j_min = 0
    j_max = 400
    k = 0.002
    n = 11
    time_max = 10.0
    time_min = 0.0
    x_max = 1.0
    x_min = 0.0

# Rank 0 prints out information about program

    if ( id == 0 ) :
        print ( "" )
        print ( "  Compute an approximate solution to the time dependent" )
        print ( "  one dimensional heat equation:" )
        print ( "" )
        print ( "    dH/dt - K * d2H/dx2 = f(x,t)" )
        print ( "" )
        print ( "  for ", x_min," = x_min < x < x_max = ", x_max )
        print ( "" )
        print ( "  and ", time_min," = time_min < t <= t_max = ", time_max )
        print ( "" )
        print ( "  Boundary conditions are specified at x_min and x_max." )
        print ( "  Initial conditions are specified at time_min." )
        print ( "" )
        print ( "  The finite difference method is used to discretize the" )
        print ( "  differential equation." )
        print ( "" )
        print ( "  This uses ", p*n," equally spaced points in X " )
        print ( "  and ", j_max," equally spaced points in time." )
        print ( "" )
        print ( "  Parallel execution is done using ", p," processors." )
        print ( "  Domain decomposition is used. " )
        print ( "  Each processor works on ", n, "nodes, ")
        print ( "  and shares some information with its immediate neighbors.")

# Computing the coordinates for N nodes on each process
# These are different for each process (x[0] and x[n+1] are not really needed)
        
    x=np.zeros(n+2)
    
    for i in range(n+2) :
        x[i] = ( (id*n+i-1)   * x_max + \
                 (p*n-id*n-i) * x_min ) / \
                 (p*n-1)

# If there's only one process, print out the coordinates
# (too many otherwise)
        
    if ( p == 1 ) :
        f = open( "x_data.txt", "w" )
        print( x[1:n+1], file=f )
        f.close()

    time = time_min

# Initialize h
    
    h = np.zeros(n+2)
    h_new = np.zeros(n+2)

    h[0] = 0.0

    for i in range(1,n+1) :
        h[i] = initial_condition( x[i], time )

    h[n+1] = 0.0

# set up deltas for time and x

    time_delta = ( time_max - time_min ) / ( j_max - j_min )
    x_delta    = ( x_max    - x_min    ) / ( p * n - 1     )

# compute courant friedrichs levy condition to assure that time steps are small enough

    cfl = k * time_delta / x_delta / x_delta 

    if ( id == 0 ) :
        print ( "" )
        print ( "UPDATE" )
        print ( "  CFL stability criterion value = ",cfl )

    if ( cfl >= 0.5 ) :
        if ( id == 0 ) :
            print ( "" );
            print ( "UPDATE - Warning!" );
            print ( "  Computation cancelled!" );
            print ( "  CFL condition failed." );
            print ( "  0.5 <= K * dT / dX / dX = ", cfl );
        return

# for single processor print out initial values for h
   
    if ( p == 1 ) :
        f = open ( "h_data.txt" , "a" )
        print ( h[1:n+1], file=f )

# loop over time steps
        
    for j in range(1,j_max+1) :
        time_new = ( (         j - j_min ) * time_max \
                   + ( j_max - j         ) * time_min ) \
                   / ( j_max     - j_min )

# send leftmost value to left neighbor
        
        if ( id > 0 ) :
            comm.send ( h[1], dest=id-1, tag=1 )

# receive rightmost value from right neighbor

        if ( id < p-1 ) :
            h[n+1] = comm.recv ( source=id+1, tag=1 )

# send rightmost value from right neighbor
            
        if ( id < p-1 ) :
            comm.send ( h[n], dest=id+1, tag=2 )

# receive leftmost value from left neighbor
            
        if ( id > 0 ) :
            h[0] = comm.recv (source=id-1, tag=2)

# compute new h values based on the ones from previous time step
            
        for i in range ( 1, n+1 ) :
            h_new[i] = h[i] \
            + ( time_delta * k / x_delta / x_delta ) * ( h[i-1] - 2.0 * h[i] + h[i+1] ) \
            + time_delta * rhs ( x[i], time )

# two of the processes (rank 0 and P-1) have extreme values that are
# deviating from the boundary conditions: replace them

        if ( 0 == id ) :
            h_new[1] = boundary_condition ( x[1], time_new )

        if ( id == p-1 ) :
            h_new[n] = boundary_condition ( x[n], time_new )

# the new time becomes the old one, and the values for H are updated
            
        time = time_new

        for i in range( 1, n+1 ) :
            h[i] = h_new[i]

# for single process run, print H again

        if ( p == 1 ) :
            print( h[1:n+1], file=f )

# close H output file after time loop
            
    if ( p == 1 ) :
        f.close()

    return
# End of update

# ------------------------------
# Main body of the main routine
#-------------------------------

comm=MPI.COMM_WORLD
id=comm.Get_rank()
p=comm.Get_size()
 
if (id==0) :
    timestamp()
    print( "" )
    print( "HEAT_MPI:" )
    print( "  Python/MPI version" )
    print( "  Solve the 1D time-dependent heat equation." )

if (id==0) :
    wtime = MPI.Wtime()     
    
update(id,p)

if (id==0) :
    wtime = MPI.Wtime() - wtime
    print(" ")
    print(" Wall clock elapsed seconds = ", wtime )

MPI.Finalize()

if (id == 0) :
    print( "" )
    print( "HEAT_MPI:" )
    print( "  Normal end of execution." )
    print( "" )
    timestamp ()

# End of main routine
~~~

Note a few features of this code:

* The data points are distributed over the fixed domain from 0 to 1.

* The number of data points is proportional to the number of processes
  employed. The more processes, the more grid points.

* This means we are looking for "weak scaling", i.e. instead of trying
  to get the same job done in a fraction of the time, we are doing a
  bigger (more accurate) job in about the same time.

* Communication is blocking and ordered. Each process sends a value to
  the left, receives one from the right, then sends one to the right
  and recives one from the left. If there is no neighbor then the
  value is replaced by boundary conditions.

* This may lead to delays, particularly with receives. For large
  number of processors and grid points, scaling may be impeded by
  it. Non-blocking communication will alleviate the problem at the
  expense of considerably more complex code.

* The boundary conditions in this example are fixed on one end and
  time-dependent on the other. Experiment with different settings by
  altering the code.

* The CFL (Courant-Friedrichs-Levy) condition that is tested limits
  the size of the time steps chosen (time_delta). If the time steps
  are chosen too large, the finite-difference approximation to the
  underlying differential equation is too inaccurate and the solution
  becomes unstable.

* The criterion is based on the condition that a time-development
  "travelling" through the domain must not have time to move to the
  neighboring gridpoints within a single time step.

* This condition forces us to choose smaller time steps when
  increasing the mesh density, and quadratically so. That makes more
  accurate computations a lot more expensive.


