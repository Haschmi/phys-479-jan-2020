---
layout: page
title: Parallel Computing on HPC systems
subtitle: Week 3, Session 1, Hour 1 : Domain Decomposition and Heat Equation
minutes: 60
---
> ##  Outline {.objectives}
> * Domain decomposition
> * General considerations
> * Finite Differences
> * Simple example : 1D Heat Equation
> * Bare Bones Implementation

This part of the markup pages is incomplete.

## Finite differences

Many of the problems in dynamics are expressed in terms of
differential equations, i.e. they express relationship between
derivatives of physical quantities with respect to space and time. In
order to solve suchg equations on a computer, the typical approach is
to discretize the continuous solution of the differential equation on
a "grid", i.e. to work with discrete values rather than an explicit
expression for the function.

We then approximate the spatial derivatives that occur in the
differential equation by expresssions based on the difference of the
functional values at neighboring points of the grid. This results in
linear equation systems that can be solved to obtain consecutive
values in time.

Details for this concern the level of approximation for the spatial
derivatives, and largely influence how many points on the grid have to
be used to approximate first and second derivatives.

In this session we will look at a simple diffusion (or heat)
equation, where the rate of change in the quantity under consideration
(say, heat) is proportional to the second derivative (curvature) of
the quantity at that point. The curvature can be thought of as the
difference between the quantity at the point and its average in an
infinitesimal neighborhood. The bigger the difference, the quicker it changes.

du/dt = k * d2u/dx2

Commonly, there are boundary conditions that fix the value and/or it's
derivatives at the boundary of the domain in question.

Let's write the above equation in terms of "finite differences",
i.e. neighboring points of the one-dimensional domain. We use the
"forward Euler scheme" for this, i.e. the time derivative is written as a difference between the next point in time and the current one. The second derivative is expressed by an approximation using the left- and right-hand neighbors and the value at the point itself:

[ u(i,n+1)-u(i,n) ] / dt = k * [ u(i+1,n) - 2 * u(i,n) + u(i-1,n) ] / (dx)^2

We used the following notations:
* index i labels a point in space, all equidistant with distance dx
* index n labels the time, starting with 0 and time steps of dt

That's as simple and Cartesian as we can make it. We can easily solve
this for the "next" time step:

u(n+1,i) = u(n,i) + F * (u(n,i+1) - 2(n,i) + u(n,i-1))

where F = k*dt/(dx)^2

This gives us a recipe on how to compute the time development of a
discretized heat distribution in one dimension. It will be pretty
inaccurate if few gridpoint in space are used, but if we choose them
dense enough, the derivatives are reasonable and the scheme will work.

## Domain decomposition

For a one-dimensional scheme this is probably best done in serial, but
for 2D and 3D problems, issue may appear rather quickly: a hundred
points in each direction of 3D space, and you have to deal with a
million grid points. The expressions for the derivatives become more
involved, and more neighbours may have to be drawn in to increase
accuracy. Too many gridpoints, and the memory requirements become too
large to be kept on a single machine.

The way to make the problem tractable may be to split up the "domain",
i.e. the space segement under consideration into several pieces, each
tackled on a different CPU, or node. This is called domain
decomposition and is the standard method of parallelizing grid-based
methods. But there is a catch: For each time step, some of the values
that are needed to compute the second derivative may not be part of
the domain a specific process is responsible for. Otherwise we could
just solve a hundred independent versions of the equation in 100
subdomains independently. It's almost the same thing, but the boundary
conditions for the subdomains change at every time step and have to be
communicated every time.

So typically, with parallel finite-difference methods, every time step
reuires communication with the nearest neighbors. For Cartesian grids
these are typically two for 1D, 4 for 2D, and 6 for 3D. The more
processes, the more subdomains, the more communication, the more
overhead. At least the communication scales linearly with the number
of processes, as not everyone has to talk with everyone. And at the
overall boundaries, the conditions are fixed.

## Explicit implementation

The following is an implemantation of the 1D heat equation that was
converted from the C version of John Burkhardt
(https://people.sc.fsu.edu/~jburkardt/c_src/heat_mpi/heat_mpi.html)

This version also includes a "source term" on the right-hand side of
the differential equation. Such source terms are common and correspond
(as the name suggests) to external time dependent sources of
heat. This term does not complicate the picture much, but we kept it
out of our introduction. I appears as the array rhs[] in the code.

~~~ {.python}
#! /usr/bin/env python3

import cmath
import time as tm
import matplotlib.pyplot as plt
import numpy as np
import sys

from mpi4py import MPI

def timestamp() :
    print ( "Local time : ", tm.ctime( tm.time() ) )
    return

def initial_condition ( x, time ):
    value = 95.0
    return value

def rhs ( x, time ) :
    value = 0.0
    return value

def boundary_condition ( x, time) :

    if ( x < 0.5 ) :
        value = 100.0 + 10.0 * np.sin ( time )
    else :
        value = 75.0
    
    return value

# Top of update routine
def update(id,p) :

    j_min = 0
    j_max = 400
    k = 0.002
    n = 11
    time_max = 10.0
    time_min = 0.0
    x_max = 1.0
    x_min = 0.0
    
    if ( id == 0 ) :
        print ( "" )
        print ( "  Compute an approximate solution to the time dependent" )
        print ( "  one dimensional heat equation:" )
        print ( "" )
        print ( "    dH/dt - K * d2H/dx2 = f(x,t)" )
        print ( "" )
        print ( "  for ", x_min," = x_min < x < x_max = ", x_max )
        print ( "" )
        print ( "  and ", time_min," = time_min < t <= t_max = ", time_max )
        print ( "" )
        print ( "  Boundary conditions are specified at x_min and x_max." )
        print ( "  Initial conditions are specified at time_min." )
        print ( "" )
        print ( "  The finite difference method is used to discretize the" )
        print ( "  differential equation." )
        print ( "" )
        print ( "  This uses ", p*n," equally spaced points in X " )
        print ( "  and ", j_max," equally spaced points in time." )
        print ( "" )
        print ( "  Parallel execution is done using ", p," processors." )
        print ( "  Domain decomposition is used. " )
        print ( "  Each processor works on ", n, "nodes, ")
        print ( "  and shares some information with its immediate neighbors.")
        
    x=np.zeros(n+2)
    
    for i in range(n+2) :
        x[i] = ( (id*n+i-1)   * x_max + \
                 (p*n-id*n-i) * x_min ) / \
                 (p*n-1)

    if ( p == 1 ) :
        f = open( "x_data.txt", "w" )
        print( x[1:n+1], file=f )
        f.close()

    time = time_min

    h = np.zeros(n+2)
    h_new = np.zeros(n+2)

    h[0] = 0.0

    for i in range(1,n+1) :
        h[i] = initial_condition( x[i], time )

    h[n+1] = 0.0

    time_delta = ( time_max - time_min ) / ( j_max - j_min )
    x_delta    = ( x_max    - x_min    ) / ( p * n - 1     )

    cfl = k * time_delta / x_delta / x_delta 

    if ( id == 0 ) :
        print ( "" )
        print ( "UPDATE" )
        print ( "  CFLF stability criterion value = ",cfl )

    if ( cfl >= 0.5 ) :
        if ( id == 0 ) :
            print ( "" );
            print ( "UPDATE - Warning!" );
            print ( "  Computation cancelled!" );
            print ( "  CFL condition failed." );
            print ( "  0.5 <= K * dT / dX / dX = ", cfl );
        return
   
    if ( p == 1 ) :
        f = open ( "h_data.txt" , "a" )
        print ( h[1:n+1], file=f )

    for j in range(1,j_max+1) :
        time_new = ( (         j - j_min ) * time_max \
                   + ( j_max - j         ) * time_min ) \
                   / ( j_max     - j_min )

        if ( id > 0 ) :
            comm.send ( h[1], dest=id-1, tag=1 )

        if ( id < p-1 ) :
            h[n+1] = comm.recv ( source=id+1, tag=1 )

        if ( id < p-1 ) :
            comm.send ( h[n], dest=id+1, tag=2 )

        if ( id > 0 ) :
            h[0] = comm.recv (source=id-1, tag=2)

        for i in range ( 1, n+1 ) :
            h_new[i] = h[i] \
            + ( time_delta * k / x_delta / x_delta ) * ( h[i-1] - 2.0 * h[i] + h[i+1] ) \
            + time_delta * rhs ( x[i], time )

        if ( 0 == id ) :
            h_new[1] = boundary_condition ( x[1], time_new )

        if ( id == p-1 ) :
            h_new[n] = boundary_condition ( x[n], time_new )

        time = time_new

        for i in range( 1, n+1 ) :
            h[i] = h_new[i]

        if ( p == 1 ) :
            print( h[1:n+1], file=f )

    if ( p == 1 ) :
        f.close()

    return
# End of update

# ------------------------------
# Main body of the main routine
#-------------------------------

comm=MPI.COMM_WORLD
id=comm.Get_rank()
p=comm.Get_size()
 
if (id==0) :
    timestamp()
    print( "" )
    print( "HEAT_MPI:" )
    print( "  Python/MPI version" )
    print( "  Solve the 1D time-dependent heat equation." )

if (id==0) :
    wtime = MPI.Wtime()     
    
update(id,p)

if (id==0) :
    wtime = MPI.Wtime() - wtime
    print(" ")
    print(" Wall clock elapsed seconds = ", wtime )

MPI.Finalize()

if (id == 0) :
    print( "" )
    print( "HEAT_MPI:" )
    print( "  Normal end of execution." )
    print( "" )
    timestamp ()

# End of main routine
~~~
(...some more explanations needed...)


